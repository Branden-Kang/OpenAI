{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Q-Learning with Keras and Gym.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IbWqUSd4dtv",
        "colab_type": "code",
        "outputId": "93ff0743-feb5-49d1-fd8c-1f7b4028034a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "EPISODES = 10\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.gamma = 0.95    # discount rate\n",
        "        self.epsilon = 1.0  # exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.99\n",
        "        self.learning_rate = 0.001\n",
        "        self.model = self._build_model()\n",
        "        self.target_model = self._build_model()\n",
        "        self.update_target_model()\n",
        "\n",
        "    \"\"\"Huber loss for Q Learning\n",
        "\n",
        "    References: https://en.wikipedia.org/wiki/Huber_loss\n",
        "                https://www.tensorflow.org/api_docs/python/tf/losses/huber_loss\n",
        "    \"\"\"\n",
        "\n",
        "    def _huber_loss(self, y_true, y_pred, clip_delta=1.0):\n",
        "        error = y_true - y_pred\n",
        "        cond  = K.abs(error) <= clip_delta\n",
        "\n",
        "        squared_loss = 0.5 * K.square(error)\n",
        "        quadratic_loss = 0.5 * K.square(clip_delta) + clip_delta * (K.abs(error) - clip_delta)\n",
        "\n",
        "        return K.mean(tf.where(cond, squared_loss, quadratic_loss))\n",
        "\n",
        "    def _build_model(self):\n",
        "        # Neural Net for Deep-Q learning Model\n",
        "        model = Sequential()\n",
        "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
        "        model.add(Dense(24, activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "        model.compile(loss=self._huber_loss,\n",
        "                      optimizer=Adam(lr=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def update_target_model(self):\n",
        "        # copy weights from model to target_model\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0])  # returns action\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = self.model.predict(state)\n",
        "            if done:\n",
        "                target[0][action] = reward\n",
        "            else:\n",
        "                # a = self.model.predict(next_state)[0]\n",
        "                t = self.target_model.predict(next_state)[0]\n",
        "                target[0][action] = reward + self.gamma * np.amax(t)\n",
        "                # target[0][action] = reward + self.gamma * t[np.argmax(a)]\n",
        "            self.model.fit(state, target, epochs=1, verbose=0)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env = gym.make('CartPole-v1') # StarGunnerNoFrameskip-v0\n",
        "    state_size = env.observation_space.shape[0]\n",
        "    action_size = env.action_space.n\n",
        "    agent = DQNAgent(state_size, action_size)\n",
        "    # agent.load(\"./save/cartpole-ddqn.h5\")\n",
        "    done = False\n",
        "    batch_size = 32\n",
        "\n",
        "    for e in range(EPISODES):\n",
        "        state = env.reset()\n",
        "        state = np.reshape(state, [1, state_size])\n",
        "        for time in range(500):\n",
        "            # env.render()\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            reward = reward if not done else -10\n",
        "            next_state = np.reshape(next_state, [1, state_size])\n",
        "            agent.remember(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            if done:\n",
        "                agent.update_target_model()\n",
        "                print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
        "                      .format(e, EPISODES, time, agent.epsilon))\n",
        "                break\n",
        "            if len(agent.memory) > batch_size:\n",
        "                agent.replay(batch_size)\n",
        "        # if e % 10 == 0:\n",
        "        #     agent.save(\"./save/cartpole-ddqn.h5\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode: 0/10, score: 22, e: 1.0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "episode: 1/10, score: 16, e: 0.93\n",
            "episode: 2/10, score: 12, e: 0.83\n",
            "episode: 3/10, score: 25, e: 0.64\n",
            "episode: 4/10, score: 25, e: 0.5\n",
            "episode: 5/10, score: 11, e: 0.45\n",
            "episode: 6/10, score: 10, e: 0.4\n",
            "episode: 7/10, score: 12, e: 0.36\n",
            "episode: 8/10, score: 15, e: 0.31\n",
            "episode: 9/10, score: 19, e: 0.25\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}