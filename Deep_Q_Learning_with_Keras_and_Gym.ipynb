{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Q-Learning with Keras and Gym.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O-eMrquRIIH",
        "colab_type": "text"
      },
      "source": [
        "# Let's do Cartpole gym\n",
        " [Cartpole gym](https://gym.openai.com/envs/CartPole-v0/) <br>\n",
        " [Keras optimizers](https://keras.io/optimizers/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OKK8FUURw7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "!pwd\n",
        "os.chdir('gdrive/Shared drives/Reinforcement Learning/Cartpole')\n",
        "!pwd\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IbWqUSd4dtv",
        "colab_type": "code",
        "outputId": "1777b7aa-0f80-41cb-dcc9-a278ea4b8b6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "EPISODES = 10\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.gamma = 0.95    # discount rate\n",
        "        self.epsilon = 1.0  # exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.99\n",
        "        self.learning_rate = 0.001\n",
        "        self.model = self._build_model()\n",
        "        self.target_model = self._build_model()\n",
        "        self.update_target_model()\n",
        "\n",
        "    \"\"\"Huber loss for Q Learning\n",
        "\n",
        "    References: https://en.wikipedia.org/wiki/Huber_loss\n",
        "                https://www.tensorflow.org/api_docs/python/tf/losses/huber_loss\n",
        "    \"\"\"\n",
        "\n",
        "    def _huber_loss(self, y_true, y_pred, clip_delta=1.0):\n",
        "        error = y_true - y_pred\n",
        "        cond  = K.abs(error) <= clip_delta\n",
        "\n",
        "        squared_loss = 0.5 * K.square(error)\n",
        "        quadratic_loss = 0.5 * K.square(clip_delta) + clip_delta * (K.abs(error) - clip_delta)\n",
        "\n",
        "        return K.mean(tf.where(cond, squared_loss, quadratic_loss))\n",
        "\n",
        "    def _build_model(self):\n",
        "        # Neural Net for Deep-Q learning Model\n",
        "        model = Sequential()\n",
        "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
        "        model.add(Dense(24, activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "        model.compile(loss=self._huber_loss,\n",
        "                      optimizer=Adam(lr=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def update_target_model(self):\n",
        "        # copy weights from model to target_model\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0])  # returns action\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = self.model.predict(state)\n",
        "            if done:\n",
        "                target[0][action] = reward\n",
        "            else:\n",
        "                # a = self.model.predict(next_state)[0]\n",
        "                t = self.target_model.predict(next_state)[0]\n",
        "                target[0][action] = reward + self.gamma * np.amax(t)\n",
        "                # target[0][action] = reward + self.gamma * t[np.argmax(a)]\n",
        "            self.model.fit(state, target, epochs=1, verbose=0)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env = gym.make('CartPole-v1') # StarGunnerNoFrameskip-v0\n",
        "    state_size = env.observation_space.shape[0]\n",
        "    action_size = env.action_space.n\n",
        "    agent = DQNAgent(state_size, action_size)\n",
        "    # agent.load(\"./save/cartpole-ddqn.h5\")\n",
        "    done = False\n",
        "    batch_size = 32\n",
        "\n",
        "    for e in range(EPISODES):\n",
        "        state = env.reset()\n",
        "        state = np.reshape(state, [1, state_size])\n",
        "        for time in range(500):\n",
        "            # env.render()\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            reward = reward if not done else -10\n",
        "            next_state = np.reshape(next_state, [1, state_size])\n",
        "            agent.remember(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            if done:\n",
        "                agent.update_target_model()\n",
        "                print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
        "                      .format(e, EPISODES, time, agent.epsilon))\n",
        "                break\n",
        "            if len(agent.memory) > batch_size:\n",
        "                agent.replay(batch_size)\n",
        "            if e % 10 == 0:\n",
        "                # agent.save(\"./save/cartpole-ddqn.h5\")\n",
        "                save_path = 'save/'\n",
        "                np.save(save_path+'cartpole-ddqn.npy', agent)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode: 0/10, score: 33, e: 0.99\n",
            "episode: 1/10, score: 16, e: 0.84\n",
            "episode: 2/10, score: 9, e: 0.77\n",
            "episode: 3/10, score: 17, e: 0.65\n",
            "episode: 4/10, score: 17, e: 0.55\n",
            "episode: 5/10, score: 14, e: 0.48\n",
            "episode: 6/10, score: 13, e: 0.42\n",
            "episode: 7/10, score: 10, e: 0.38\n",
            "episode: 8/10, score: 13, e: 0.33\n",
            "episode: 9/10, score: 13, e: 0.29\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHQ2Vmv8j4vh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import tables\n",
        "# h5file = tables.open_file(f\"./save/cartpole-ddqn.h5\", \"w\", driver=\"H5FD_CORE\")\n",
        "# import numpy\n",
        "# a = h5file.create_array(h5file.root, \"array\", numpy.zeros((300, 300)))\n",
        "# print(a)\n",
        "# h5file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QErfxoolS3nG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Plot the data in Google Colab\n",
        "# # If using Google Drive\n",
        "# # from google.colab import drive\n",
        "# # drive.mount(\"/content/gdrive\")\n",
        "\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# def my_custom_func():\n",
        "#     # your code\n",
        "#     return\n",
        "# from keras.models import load_model\n",
        "# model = load_model('my_model.h5', )\n",
        "\n",
        "# deepq_reward = np.load(f\"./save/cartpole-ddqn.npy\",allow_pickle=True)\n",
        "\n",
        "# # import tables\n",
        "# # deepq_reward = tables.open_file(f\"./save/cartpole-ddqn.h5\", mode='w', title=\"Test Array\")\n",
        "\n",
        "# fig, ax = plt.subplots()\n",
        "# deepq_plot = ax.plot(deepq_reward, label=\"DQN\")\n",
        "\n",
        "# # Show legend\n",
        "# leg = ax.legend()\n",
        "\n",
        "# # Set line width and color\n",
        "# plt.setp(deepq_plot, linewidth=0.2, color='b')\n",
        "\n",
        "# # Set grid width\n",
        "# ax.grid(linewidth=0.1)\n",
        "\n",
        "# # Add x-axis label\n",
        "# ax.set_xlabel(\"episodes\")\n",
        "\n",
        "# # Save locally\n",
        "# fig.savefig(\"carpole.png\", dpi=600)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}